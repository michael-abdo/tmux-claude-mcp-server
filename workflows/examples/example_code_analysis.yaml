name: "Code Analysis Pipeline"
version: "1.0"
description: "Analyze code, run tests, and fix issues"

settings:
  poll_interval: 5
  timeout: 300
  instance_role: "specialist"
  workspace_mode: "isolated"

stages:
  - id: "setup"
    name: "Setup Analysis Environment"
    prompt: |
      You are a code analysis specialist. Your job is to:
      1. Examine the codebase structure
      2. Identify the main programming language
      3. List available test commands
      
      Start by running 'ls -la' to see the project structure.
      
      WHEN YOU ARE DONE YOU MUST SAY "***SETUP_COMPLETE***"
    
    trigger_keyword: "***SETUP_COMPLETE***"
    
    on_success:
      # Run multiple analysis scripts
      - action: "run_script"
        script: "find . -name '*.js' -o -name '*.py' -o -name '*.ts' | wc -l"
        capture_output: true
        output_var: "file_count"
      
      - action: "log"
        message: "Found ${file_count} source files"
      
      - action: "send_prompt"
        target: "same_instance"
        next_stage: "analyze_code"

  - id: "analyze_code"
    name: "Deep Code Analysis"
    prompt: |
      Now perform a detailed code analysis:
      1. Check for common issues (unused variables, missing error handling)
      2. Look for performance bottlenecks
      3. Identify areas that need refactoring
      
      Focus on the src/ directory.
      
      WHEN YOU ARE DONE YOU MUST SAY "***ANALYSIS_COMPLETE***"
    
    trigger_keyword: "***ANALYSIS_COMPLETE***"
    
    on_success:
      # Save analysis results
      - action: "save_file"
        path: "./reports/analysis_${workflow.run_id}.txt"
        content: "${stage.output}"
      
      # Run tests to check current state
      - action: "run_script"
        script: "npm test"
        timeout: 120
        capture_output: true
        output_var: "test_results"
        on_failure: "continue"
      
      # Check if tests passed
      - action: "conditional"
        condition: "${test_results.exit_code} === 0"
        if_true:
          - action: "log"
            message: "All tests passing! Moving to optimization phase."
          - action: "send_prompt"
            target: "same_instance"
            next_stage: "optimize_code"
        if_false:
          - action: "log"
            message: "Tests failing. Need to fix issues first."
          - action: "send_prompt"
            target: "same_instance"
            next_stage: "fix_tests"

  - id: "fix_tests"
    name: "Fix Failing Tests"
    prompt: |
      The tests are failing with the following output:
      ${test_results.stderr}
      
      Please fix the failing tests. Focus on:
      1. Syntax errors
      2. Import issues
      3. Assertion failures
      
      WHEN YOU ARE DONE YOU MUST SAY "***TESTS_FIXED***"
    
    trigger_keyword: "***TESTS_FIXED***"
    
    on_success:
      # Re-run tests
      - action: "run_script"
        script: "npm test"
        expect_exit_code: 0
        capture_output: true
        output_var: "fixed_test_results"
      
      - action: "log"
        message: "Tests now passing!"
      
      - action: "send_prompt"
        target: "same_instance"
        next_stage: "optimize_code"

  - id: "optimize_code"
    name: "Optimize Performance"
    prompt: |
      Now that tests are passing, optimize the code for better performance:
      1. Look for O(nÂ²) algorithms that could be O(n)
      2. Check for unnecessary database queries
      3. Identify memory leaks or inefficient data structures
      
      Make improvements while ensuring tests still pass.
      
      WHEN YOU ARE DONE YOU MUST SAY "***OPTIMIZATION_COMPLETE***"
    
    trigger_keyword: "***OPTIMIZATION_COMPLETE***"
    
    on_success:
      # Run performance benchmark
      - action: "run_script"
        script: "./scripts/benchmark.sh"
        capture_output: true
        output_var: "benchmark_results"
        on_failure: "continue"
      
      # Generate final report
      - action: "template"
        template: |
          # Code Analysis Report
          
          ## Summary
          - Files analyzed: ${file_count}
          - Tests status: ${fixed_test_results.exit_code === 0 ? 'Passing' : 'Failed'}
          - Workflow duration: ${(Date.now() - workflow.start_time) / 1000}s
          
          ## Optimizations Applied
          ${stage.output}
          
          ## Benchmark Results
          ${benchmark_results.stdout}
          
          Generated by workflow: ${workflow.name} (${workflow.run_id})
        output_var: "report_content"
      
      - action: "save_file"
        path: "./reports/final_report_${workflow.run_id}.md"
        content: "${report_content}"
      
      - action: "log"
        message: "Analysis complete! Report saved."
      
      - action: "complete_workflow"